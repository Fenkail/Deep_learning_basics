## 思考题

> 为什么深度学习中没有最速下降法（steepestdescent）？

最速下降法也叫梯度下降法，沿着梯度负方向进行下降，按其方式和名称也*应该是*寻找极值的最优方式。该方式在一定的小范围内或者较为简单的优化空间中寻找极值点较快，并且两次的搜索方向是相互正交的，但对于深度学习而言，全局的优化上并非最快，也容易限于局部最优解。

## 实践题

> ⾃选2种或以上⾃⼰感兴趣的优化算法，并针对同⼀问题给出他们的收敛轨迹，⽐较优劣。
>
> f(x,y) = (1.5-x+x*y)**2+(2.25-x+x*y*y)**2+(2.625-x+x*y*y*y)**2

- 针对上述的函数，进行优化，本次选择momentum、ada_grad、rmsprop和adam进行分析。
- 各优化器的学习率设为0.1，各种初值取常用默认值，迭代次数设为了8000保证各优化器的结果收敛
- 各优化器的实现和求导采用了手算和代码实现（可能会有点问题)

### 1.初值x,y为（1,1）

| 方法     | 收敛后的x          | 收敛后的y           | 最终的f(x,y)        |
| -------- | ------------------ | ------------------- | ------------------- |
| momentum | 2.453641757046819  | 0.3640099461540556  | 0.10234324422831596 |
| ada_grad | 2.253175804058527  | 0.27674778810676887 | 0.22154337644010297 |
| rmsprop  | 2.4602577645847283 | 0.36548452495151923 | 0.09888589331311201 |
| adam     | 2.452700221692956  | 0.36432855062816843 | 0.10321760743870897 |

### 2.初值x,y为（0,0）

| 方法     | 收敛后的x          | 收敛后的y            | 最终的f(x,y)        |
| -------- | ------------------ | -------------------- | ------------------- |
| momentum | 2.453641757046819  | 0.3640099461540556   | 0.10234324422831596 |
| ada_grad | 1.3888429703051088 | -0.07132510353344619 | 2.280795128719702   |
| rmsprop  | 2.4749397486190206 | 0.37075432515744366  | 0.09285573380376803 |
| adam     | 2.4479127958196307 | 0.371190997211347    | 0.1123401182314019  |

### 3.初值x,y为（-1.5,1.5）

| 方法     | 收敛后的x          | 收敛后的y          | 最终的f(x,y)        |
| -------- | ------------------ | ------------------ | ------------------- |
| momentum | 2.4536417570468187 | 0.3640099461540554 | 0.10234324422831617 |
| ada_grad | -2.536446550299608 | 1.2783705714316698 | 1.060523105965509   |
| rmsprop  | -7.378938068174955 | 1.1164709245771998 | 0.6664788993506108  |
| adam     | 2.453641757046818  | 0.364009946154055  | 0.10234324422831653 |

### 4.初值x,y为（2,-1）

| 方法     | 收敛后的x          | 收敛后的y            | 最终的f(x,y)        |
| -------- | ------------------ | -------------------- | ------------------- |
| momentum | 2.453641757046816  | 0.36400994615405435  | 0.10234324422831728 |
| ada_grad | 1.8204555366498807 | -0.12067479271551981 | 1.1418899761134793  |
| rmsprop  | 2.4457530506461924 | 0.36137741834455017  | 0.10595264404054908 |
| adam     | 2.453686129569168  | 0.3639845300969695   | 0.10229500934275772 |

